![image-20210627095503924](C:\Users\Echo\AppData\Roaming\Typora\typora-user-images\image-20210627095503924.png)

ngx_events_module模块管理当前正在使用的事件驱动模块（决定使用epoll还是select）

核心是ngx_process_events_and_timers方法（事件的处理分发）

每个worker进程都在该方法循环处理事件

```
传统的Web服务器是不完全的事件驱动，其只局限于TCP连接建立，关闭事件，
连接之后关闭之前的所有操作都是按序执行操作的批处理模式。连接建立后会始终占用系统资源
对于nginx来说  每个模块的（某个函数）是事件消费者   只会被短期调用
只有事件收集/分发器（每个模块的ngx_process_events_and_timers方法）才占用cpu
每个事件消费者不能有阻塞行为
```

```
请求的多阶段异步处理
将请求按照事件触发方式分为多阶段
每个阶段触发不同的回调函数去处理
多阶段异步+事件驱动架构  使每个进程都全力运转，减少了并发处理事件的数目
```

阶段划分的依据/方法

```
1.将阻塞进程的方法分为两阶段（非阻塞方法调用+结果返回事件）例如send函数
2.如果有的操作不支持结果返回，那么就只能按照执行时间拆分  
例如 读取10MB的文件分成1000份
3.无阻塞方法调用的代码也有可能阻塞进程，  使用定时器循环检查标志

```

请求的多阶段全异步的处理流程的具体体现

```
将处理可读-接受请求行-接受请求头-处理包体-发送响应等分为了很多的阶段

交给了每个模块去处理  每个模块是消费者  只短暂的占用cpu 

多阶段全异步相当于给每个主进程找了很多工人（module）    具体的任务交给工人去做

主线程只负责分配下去具体的任务 
而不同的方法间通过判断模块标志位  ngx_ok  ngx_again等决定下一步的执行流程
```

事件处理框架概述

```
事件处理框架要解决的问题就是如何收集/管理/分发事件
事件主要是(TCP)网络事件和定时器
事件驱动机制的别名又称I/O多路复用
核心模块ngx_enents_mudule的作用是解析events{}配置块  管理存储配置项的结构体
事件模块ngx_event_core_moduel决定使用哪种事件驱动机制
另外还有九个事件模块分别表示九种事件驱动机制 epoll/poll/select/devpoll等
事件模块的通用接口{
create_conf   //创建存储配置项参数的结构体
init_conf     //从和处理当前时间模块感兴趣的配置项
action        //每个事件模块要是是是实现的方法 包括add/del/init/done等
}ngx_event_module_t
```

事件的定义

```
每个事件都由ngx_event_t结构体表示   重要成员如下
{    data      //通常是ngx_connection_t连接对象
write          //可写标志位
ngx_event_header_pt hander  //***表示事件发生时的处理方法，每个事件消费模块都会重新实现它
}
模块只要处理事件就必须设置handler回调方法 

事件的创建
连接池的每一个连接都自动的对应一个些事件和读事件
```

连接的定义

```
ngx_connection_t表示被动连接(nginx作为服务器接受的连接)
至少对应一个写事件和读事件
ngx_peer_connection_t表示主动连接（nginx主动发起的连接）
其包含了ngx_connection_t

ngx_connection_t{   结构体成员
ngx_event_t   //read读事件
ngx_event_t   //write写事件
fd       //套接字句柄
pool       //一个连接对应的内存池
recv     //接受方法
send      //发送方法
recv_chain   //接受方法
send_chain     //发送方法
...
}

ngx_peer_connection_t{   //主动连接结构体
connection    //使用了connection的大部分成员
sockaddr      //远端服务器的socket地址
...
}

ngx_connection_t连接池
ngx_connection_t结构体都是在启动阶段就在ngx_cycle_t 中分配好的  
```

![image-20210627150823090](C:\Users\Echo\AppData\Roaming\Typora\typora-user-images\image-20210627150823090.png)

```
获取/释放ngx_connection_t结构体的两种方法
ngx_get_connection
ngx_free_connection
```

ngx_events_module核心模块

```
功能就是定义新的事件类型
模块都是通过配置项定制功能
该模块只对events块配置项生效
只会在出现events配置项后调用各事件模块去解析events{...}块内的配置项
使用ngx_events_block函数   （事件块）  //同理也有ngx_http_block

管理所有事件模块的配置项
每个模块都建立自己的配置项结构体，存储感兴趣的配置项在nginx.conf中对应的参数
通过宏获取在create_conf中分配的结构体的指针（define a b （用a代替b））
#define ngx_event_get_conf(conf_ctx,module)  \
（*（ngx_get_conf(conf_ctx,ngx_events_module)）  (module.ctx_index)

管理事件模块
ngx_events_block  配置项结构体指针的保存
1.初始化所有事件模块的ctx_index序号
2.分配数组，存储所有事件模块的配置项结构体指针
3.调用所有模块的create_conf方法
4.为所有事件模块解析nginx.conf配置方法
5.调用模块的init_conf方法
```

ngx_event_core_module事件模块

```
功能：创建连接池&决定使用哪些事件驱动模块&初始化要使用的事件模块
7个配置项{
worker_connection  //连接池的大小
connections        //连接池的大小
use            //使用哪一个时间模块作为驱动机制
multi_accept  //使一个工作进程可接受多个连接
accept_mutex   //负载均衡锁
accept_mutex_delay  //启用负载均衡锁后，延迟一定毫秒后再处理新连接事件
debug_connection  //打印debug级别的日志
}

ngx_event_core_nodule{   //方法
event_core_module_init  
ngx_event_module_init    //event_core_module模块启动过程主要工作
ngx_event_process_init
}
```

epoll事件驱动模块

```
实现原理：在内核中申请了简易的文件系统
将原先的select/poll操作分成了三步骤
epoll_create   创建epoll对象
epoll_ctl      添加监听套接字
epoll_wait     收集发生事件的连接

linux对epoll的支持（epoll就是event_poll的简写）
在epoll_create方式时  内核会创建一个eventpoll结构体   主要数据成员如下
{ 
br_root rbr //存储着所有添加到epoll的事件
list_head rblist  //保存着将要通过epoll返回给用户的，满足条件的事件
}
每个事件都会对应这一个epitem结构体{
rbn成员在红黑树中监听
rdlink成员是要通过epoll_wait返回的节点
}

检查epoll_wait是否又发生事件的连接时  先检查eventpoll的rblist是否有元素

nginx默认使用ET模式  只支持非阻塞  对于带来的时候只题型一次  
LT对一个事件对应的套接字缓冲区数据未处理完时 总能够重新获取
ET的效率更高  但是开发起来要耿复杂
```

ngx_epoll_module模块的实现

```
两个配置项：
epoll_events   //
worker_aio_requests  //

配置项结构体ngx_epoll_conf_t{  }   //对应两个配置项
ngx_event_module_ctx{
epoll_create_conf     //只是为了解析配置项
epoll_init_conf
  action{  //十个事件驱动模块相关的动作
  
       }
}

init接口的ngx_epoll_init方法

```

定时器事件

```
与网络事件不同  网络事件是由内核触发的
nginx定时器是由Nginx自身实现的
事件定时器的组织？超时后定时器的触发？时间管理方法？

定时器通过红黑树实现
ngx_event_timer_rbtree 包含了所有定时器事件
每个节点都是ngx_event_t的timer成员
```

事件驱动框架的处理流程

```
接受新连接事件的回调函数  ngx_event_accept{
调用acccept函数尝试建立连接
从连接池获取一个连接对象connection_t
为连接对象建立内存池
设置套接字为非阻塞
将连接的读时间加到epoll的事件驱动模块
调用监听对象的handler回调方法
}

惊群问题
在内核收到TCP的syn包时，会激活所有的休眠的worker子进程
应限制在某一个时刻只有一个子进程监听web端口
ngx_accept_mutex实际上是Nginx进程间的同步锁

负载均衡
accept_mutex实现worker进程间的负载均衡

事件框架的处理流程
ngx_process_events_and_timers负责事件的处理分发
核心操作有三个{
调用事件驱动模块的process_events方法  处理网络事件
ngx_event_process_posted 处理post事件队列的事件
event_expire_timer处理定时器事件
}

文件的异步IO
epoll_module模块与文件异步IO配置提供服务
glic库的异步IO是基于多线程实现  不是真正意义的异步IO 真正的异步IO是由Linux内核实现的
在内核完成磁盘操作后，通知进程，使得磁盘文件的处理和网络事件的处理一样高效
当大量读时间堆积到IO设备时，会发挥出内核“电梯算法”的优势

ngx_epoll_aio_init方法会把异步IO和epoll结合起来

```



